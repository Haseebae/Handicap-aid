{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui as pag\n",
    "import time\n",
    "\n",
    "# media pipe dependencies\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from scipy.spatial import distance as dist\n",
    "from datetime import datetime\n",
    "\n",
    "# audio file dependencies\n",
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "import pyperclip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "media pipe variables and functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAMERA = 1 # Usually 0, depends on input device(s)\n",
    "\n",
    "# Optionally record the video feed to a timestamped AVI in the current directory\n",
    "RECORDING = False\n",
    "FPS = 10\n",
    "RECORDING_FILENAME = str(datetime.now()).replace('.','').replace(':','') + '.avi'\n",
    "\n",
    "EYE_BLINK_HEIGHT = .15\n",
    "EYE_OPEN_HEIGHT = .25\n",
    "MOUTH_OPEN_HEIGHT = .2\n",
    "\n",
    "winkedR = False\n",
    "winkedL = False\n",
    "\n",
    "mouth_open = False\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect_ratio(top, bottom, right, left):\n",
    "  height = dist.euclidean([top.x, top.y], [bottom.x, bottom.y])\n",
    "  width = dist.euclidean([right.x, right.y], [left.x, left.y])\n",
    "  return height / width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_frame(image, face_landmarks):\n",
    "  mp_drawing.draw_landmarks(\n",
    "      image=image,\n",
    "      landmark_list=face_landmarks,\n",
    "      connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "      landmark_drawing_spec=None,\n",
    "      connection_drawing_spec=mp_drawing_styles\n",
    "      .get_default_face_mesh_tesselation_style())\n",
    "  mp_drawing.draw_landmarks(\n",
    "      image=image,\n",
    "      landmark_list=face_landmarks,\n",
    "      connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "      landmark_drawing_spec=None,\n",
    "      connection_drawing_spec=mp_drawing_styles\n",
    "      .get_default_face_mesh_contours_style())\n",
    "  mp_drawing.draw_landmarks(\n",
    "      image=image,\n",
    "      landmark_list=face_landmarks,\n",
    "      connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "      landmark_drawing_spec=None,\n",
    "      connection_drawing_spec=mp_drawing_styles\n",
    "      .get_default_face_mesh_iris_connections_style())\n",
    "  frame = cv2.flip(image, 1) # Flip image horizontally\n",
    "  \n",
    "  cv2.imshow('face', frame)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "speech to text dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpeakText(command):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.say(command)\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio(): \n",
    "    spoken_text = []\n",
    "\n",
    "    while True:\n",
    "        with sr.Microphone() as source2:\n",
    "            r.adjust_for_ambient_noise(source2, duration = 0.5)\n",
    "            audio2 = r.listen(source2)\n",
    "            try:\n",
    "                MyText = r.recognize_google(audio2)\n",
    "                MyText = MyText.lower()\n",
    "                if MyText.endswith('terminate'):\n",
    "                    print(\"Stopping...\")\n",
    "                    words = MyText.lower().split()\n",
    "                    words.pop()\n",
    "                    new_text = \" \".join(words)\n",
    "                    print(\"parrot says : \", new_text)\n",
    "                    spoken_text.append(new_text)\n",
    "                    pyperclip.copy(new_text)\n",
    "                    time.sleep(3)\n",
    "                    pag.hotkey('command', 'v')\n",
    "                    break\n",
    "                print(\"parrot says : \", MyText)\n",
    "                spoken_text.append(MyText)\n",
    "                pyperclip.copy(MyText)\n",
    "                time.sleep(3)\n",
    "                pag.hotkey('command', 'v')\n",
    "\n",
    "            except sr.UnknownValueError:\n",
    "                print(\"Could not understand audio, please try again.\")\n",
    "            except sr.RequestError as e:\n",
    "                print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mouth open 0.3799479600367971\n",
      "result2:\n",
      "{   'alternative': [   {'confidence': 0.92995489, 'transcript': 'terminate'},\n",
      "                       {'transcript': 'Terminator'},\n",
      "                       {'transcript': 'permanent'},\n",
      "                       {'transcript': 'terminated'},\n",
      "                       {'transcript': 'terminate a'}],\n",
      "    'final': True}\n",
      "Stopping...\n",
      "parrot says :  \n",
      "L wink 0.12401318667673797\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/haseeb/Downloads/main.ipynb Cell 12\u001b[0m in \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/haseeb/Downloads/main.ipynb#X15sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mL wink\u001b[39m\u001b[39m\"\u001b[39m, eyeL_ar)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/haseeb/Downloads/main.ipynb#X15sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m   winkedL \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/haseeb/Downloads/main.ipynb#X15sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m   pag\u001b[39m.\u001b[39;49mclick()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/haseeb/Downloads/main.ipynb#X15sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m   pag\u001b[39m.\u001b[39msleep(\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/haseeb/Downloads/main.ipynb#X15sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m mouth_inner_top \u001b[39m=\u001b[39m face[\u001b[39m13\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.10/site-packages/pyautogui/__init__.py:599\u001b[0m, in \u001b[0;36m_genericPyAutoGUIChecks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m failSafeCheck()\n\u001b[1;32m    598\u001b[0m returnVal \u001b[39m=\u001b[39m wrappedFunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 599\u001b[0m _handlePause(kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39m_pause\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m))\n\u001b[1;32m    600\u001b[0m \u001b[39mreturn\u001b[39;00m returnVal\n",
      "File \u001b[0;32m~/anaconda3/envs/DS/lib/python3.10/site-packages/pyautogui/__init__.py:643\u001b[0m, in \u001b[0;36m_handlePause\u001b[0;34m(_pause)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[39mif\u001b[39;00m _pause:\n\u001b[1;32m    642\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(PAUSE, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(PAUSE, \u001b[39mfloat\u001b[39m)\n\u001b[0;32m--> 643\u001b[0m     time\u001b[39m.\u001b[39;49msleep(PAUSE)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(CAMERA)\n",
    "screen_w, screen_h = pag.size()\n",
    "\n",
    "# to get output video, set RECORDING to true\n",
    "if RECORDING:\n",
    "  frame_size = (int(cap.get(3)), int(cap.get(4)))\n",
    "  recording = cv2.VideoWriter(\n",
    "    RECORDING_FILENAME, cv2.VideoWriter_fourcc(*'MJPG'), FPS, frame_size)\n",
    "\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as face_mesh:\n",
    "  while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    #image = cv2.flip(image,1)\n",
    "    if not success: \n",
    "      break\n",
    "\n",
    "    image.flags.writeable = False\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(image)\n",
    "\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    if results.multi_face_landmarks and len(results.multi_face_landmarks) > 0:\n",
    "      face_landmarks = results.multi_face_landmarks[0]\n",
    "      face = face_landmarks.landmark\n",
    "      image_h, image_w, _ = image.shape\n",
    "\n",
    "      for id, landmark in enumerate(face[1:3]): #Looping through the coordinates of mesh\n",
    "            x = int((1-landmark.x) * image_w)\n",
    "            y = int(landmark.y * image_h)\n",
    "            cv2.circle(image, (x,y), 3, (0,255,0)) #the points, centre of circle is x and y, 3 channels, which color the cirlce should be\n",
    "            if id==1:\n",
    "                screen_x =  (screen_w/ image_w)*x\n",
    "                screen_y =  (screen_h/ image_h)*y\n",
    "                pag.moveTo(screen_x,screen_y)\n",
    "\n",
    "      eyeR_top = face[159]\n",
    "      eyeR_bottom = face[145]\n",
    "      eyeR_inner = face[133]\n",
    "      eyeR_outer = face[33]\n",
    "      eyeR_ar = get_aspect_ratio(eyeR_top, eyeR_bottom, eyeR_outer, eyeR_inner)\n",
    "\n",
    "      eyeL_top = face[386]\n",
    "      eyeL_bottom = face[374]\n",
    "      eyeL_inner = face[362]\n",
    "      eyeL_outer = face[263]\n",
    "      eyeL_ar = get_aspect_ratio(eyeL_top, eyeL_bottom, eyeL_outer, eyeL_inner)\n",
    "      eyeA_ar = (eyeR_ar + eyeL_ar) / 2\n",
    "  \n",
    "      if eyeR_ar < EYE_BLINK_HEIGHT:\n",
    "        if eyeL_ar > EYE_OPEN_HEIGHT:\n",
    "          print(\"R wink\", eyeR_ar)\n",
    "          winkedR = True\n",
    "          pag.rightClick()\n",
    "          pag.sleep(1)\n",
    "      elif eyeL_ar < EYE_BLINK_HEIGHT and eyeR_ar > EYE_OPEN_HEIGHT:\n",
    "        print(\"L wink\", eyeL_ar)\n",
    "        winkedL = True\n",
    "        pag.click()\n",
    "        pag.sleep(1)\n",
    "        \n",
    "      mouth_inner_top = face[13]\n",
    "      mouth_inner_bottom = face[14]\n",
    "      mouth_inner_right = face[78]\n",
    "      mouth_inner_left = face[308]\n",
    "      mouth_inner_ar = get_aspect_ratio(\n",
    "        mouth_inner_top, mouth_inner_bottom, mouth_inner_right, mouth_inner_left)\n",
    "\n",
    "      mouth_open = mouth_inner_ar > MOUTH_OPEN_HEIGHT\n",
    "      if mouth_open:\n",
    "        print(\"mouth open\", mouth_inner_ar)\n",
    "        get_audio()\n",
    "\n",
    "      draw_frame(image, face_landmarks)\n",
    "      if RECORDING:\n",
    "        recording.write(image)\n",
    "\n",
    "    # Type 'q' on the video frame to quit\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "      break\n",
    "\n",
    "if RECORDING:\n",
    "  recording.release()\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
